# Training configuration using pre-interpolated data (correction-only mode)
# The model learns to correct interpolation errors rather than full coarse-to-fine mapping

data:
  # Path to pre-interpolated solution on fine mesh
  interpolated_file: '/home/qg7/Vertex/paraview/90-30_to_360-120_interpolated_linear.exo'
  
  # Path to ground truth fine mesh solution
  fine_file: '/home/qg7/Vertex/dataset/002-Re-148_3-AC-beta-10000-Helios/360-120/cropped.e'
  
  # Timestep offset: interpolated[t + offset] = fine[t]
  # For your data: interpolated[ts+8] = fine[ts], so offset=8
  timestep_offset: 8 
  
  # Caching (optional, usually not needed for pre-interpolated data)
  use_cache: false
  cache_dir: './cache'

model:
  type: 'encoder_decoder'                    # Options: gnn, encoder_decoder, mlp
  hidden_channels: 96 
  num_layers: 4
  dropout: 0.1
  k_neighbors: 12                 # Number of neighbors for graph construction

training:
  batch_size: 1                  # Graph data typically uses batch_size=1
  learning_rate: 0.0005
  weight_decay: 0.00001
  num_epochs: 120
  save_every: 10
  output_dir: './outputs_correction'
  validation_split: 0.2          # Use 20% of timesteps for validation
  residual_learning: true        # Train model to predict (fine - interpolated) residuals
  residual_normalization: true   # Normalize residual targets: (residual - mean) / std (stats saved in checkpoint)
  residual_norm_eps: 1.0e-8      # Std clamp to avoid divide-by-zero
  smoothness_lambda: 1.0e-4         # Graph smoothness penalty weight on predicted correction (e.g., 1e-4)

# Note: This mode trains the model to learn corrections/residuals
# Input: Pre-interpolated fields on fine mesh
# Output: Corrected/refined fields
# Target: Ground truth fine mesh fields
